{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guyyariv/DL_ex3/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ws54rEXFwINa"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "import imageio\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "## load mnist dataset\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "fdOzvKUAlgDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "root = './data'\n",
        "if not os.path.exists(root):\n",
        "    os.mkdir(root)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "# training set and train data loader\n",
        "trainset = torchvision.datasets.MNIST(\n",
        "    root='../input', train=True, download=True, transform=transform\n",
        ")\n",
        "\n",
        "trainset_mlp, trainset_autoencoder, two_images = torch.utils.data.random_split(trainset, [200, len(trainset) - 202, 2])\n",
        "train_loader_mlp = DataLoader(\n",
        "    trainset_mlp, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "train_loader_autoencoder = DataLoader(\n",
        "    trainset_autoencoder, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "train_loader_two_images = DataLoader(\n",
        "    two_images, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "trainloader = DataLoader(\n",
        "    trainset, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "# validation set and validation data loader\n",
        "testset = torchvision.datasets.MNIST(\n",
        "    root='../input', train=False, download=True, transform=transform\n",
        ")\n",
        "testloader = DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "WNnvz6ydle57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(y_true, y_pred, is_training=False):\n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "\n",
        "    return torch.sum(y_true == y_pred) / len(y_true)"
      ],
      "metadata": {
        "id": "bzM6NnwtbRQI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder architecture"
      ],
      "metadata": {
        "id": "lj3ywVOyl3vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, lat_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        ## encoder layers ##\n",
        "        # conv layer (depth from 3 --> 32), 3x3 kernels\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        # conv layer (depth from 32 --> 16), 3x3 kernels\n",
        "        self.conv2 = nn.Conv2d(32, 16, 3, padding=1)\n",
        "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(1024, 256)\n",
        "        self.fc = nn.Linear(256, lat_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## encode ##\n",
        "        # add hidden layers with relu activation function\n",
        "        # and maxpooling after\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        # add second hidden layer\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x).reshape((x.shape[0], 1024))  # compressed representation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, lat_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        ## decoder layers ##\n",
        "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
        "        self.fc = nn.Linear(lat_dim, 256)\n",
        "        self.fc1 = nn.Linear(256, 1024)\n",
        "        self.t_conv1 = nn.ConvTranspose2d(16, 32, 2, stride=2)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(32, 1, 2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## decode ##\n",
        "        x = F.relu(self.fc(x))\n",
        "        x = F.relu(self.fc1(x)).reshape((x.shape[0], 16, 8, 8))\n",
        "        # add transpose conv layers, with relu activation function\n",
        "        x = F.relu(self.t_conv1(x))\n",
        "        # output layer (with sigmoid for scaling from 0 to 1)\n",
        "        x = torch.sigmoid(self.t_conv2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self, lat_dim):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder(lat_dim)\n",
        "        self.decoder = Decoder(lat_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "IRaWNc6rl0gg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Autoencoder"
      ],
      "metadata": {
        "id": "rWSV9gGcl5GG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_autoencoder(model, latent_dim, num_epochs=10, learning_rate=1e-3, trainset=trainset_autoencoder, train_loader=train_loader_autoencoder):\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.BCELoss()  # binary cross entropy loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    outputs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        loss_counter = 0.0\n",
        "        counter = 0\n",
        "        for i, data in tqdm(enumerate(train_loader), total=int(len(trainset)/train_loader.batch_size)):\n",
        "            img, _ = data\n",
        "            recon = model(img)\n",
        "            loss = criterion(recon, img)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            loss_counter += loss.item()\n",
        "            counter += 1\n",
        "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, loss_counter / counter))\n",
        "        loss_list.append(loss_counter / counter)\n",
        "        outputs.append((i, img, recon),)\n",
        "    plt.plot(loss_list)\n",
        "    plt.legend('loss', 'epochs')\n",
        "    plt.show()\n",
        "    torch.save(model.encoder.state_dict(), f'encoder_model_{latent_dim}.pth')\n",
        "    torch.save(model.decoder.state_dict(), f'decoder_model_{latent_dim}.pth')\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "r966v9E9l-Zg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_autoencoder(latent_dim):\n",
        "    model = ConvAutoencoder(latent_dim)\n",
        "    max_epochs = 40\n",
        "    outputs = train_autoencoder(model, latent_dim=latent_dim, num_epochs=max_epochs)\n",
        "\n",
        "    for k in range(0, max_epochs):\n",
        "        plt.figure(figsize=(9, 2))\n",
        "        imgs = outputs[k][1].detach().numpy()\n",
        "        recon = outputs[k][2].detach().numpy()\n",
        "        for i, item in enumerate(imgs):\n",
        "            if i >= 9: break\n",
        "            plt.subplot(2, 9, i+1)\n",
        "            plt.imshow(item[0], cmap='gray')\n",
        "\n",
        "        for i, item in enumerate(recon):\n",
        "            if i >= 9: break\n",
        "            plt.subplot(2, 9, 9+i+1)\n",
        "            plt.imshow(item[0], cmap='gray')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tnyMuZHJAjrQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_autoencoder(latent_dim=5)\n",
        "run_autoencoder(latent_dim=8)\n",
        "run_autoencoder(latent_dim=10)\n",
        "run_autoencoder(latent_dim=14)\n",
        "run_autoencoder(latent_dim=20)"
      ],
      "metadata": {
        "id": "lqqdwrkaAngL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decorrelation"
      ],
      "metadata": {
        "id": "OWH5sEfHC0ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latents = [5, 8, 10, 14,20]\n",
        "result = dict()\n",
        "for latent_dim in latents:\n",
        "\n",
        "    encoder = Encoder(latent_dim)\n",
        "    encoder.load_state_dict(copy.deepcopy(torch.load(f\"encoder_model_{latent_dim}.pth\", device)))\n",
        "    \n",
        "    trainloader = DataLoader(trainset, batch_size=1000, shuffle=True)\n",
        "    for i, data in enumerate(trainloader):\n",
        "        images, _ = data\n",
        "        latent = encoder(images)\n",
        "        if i == 0:\n",
        "          break\n",
        "    corr = torch.corrcoef(latent.T).detach().numpy()\n",
        "    result[latent_dim] = np.linalg.norm(corr)\n",
        "    print(f'{latent_dim}: {result[latent_dim]}')\n",
        "plt.figure()\n",
        "plt.plot(list(result.keys()),list(result.values()))\n",
        "plt.scatter(list(result.keys()),list(result.values()))\n",
        "plt.xlabel('latent dim')\n",
        "plt.ylabel('norm of cross cor')\n",
        "plt.title('latent dim size correlation')"
      ],
      "metadata": {
        "id": "YvNPZ1NiC5EM",
        "outputId": "8c77991c-6b57-421e-8b67-a3766be3cdfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5: 2.375281572341919\n",
            "8: 3.338815450668335\n",
            "10: 3.7216219902038574\n",
            "14: 5.005959987640381\n",
            "20: 6.456265449523926\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'latent dim size correlation')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Architecture"
      ],
      "metadata": {
        "id": "l6wj-T-6mGVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, input_dim=latent_dim):\n",
        "        super(Mlp, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, input_dim * 2)\n",
        "        self.linear2 = nn.Linear(input_dim * 2, input_dim)\n",
        "        self.linear3 = nn.Linear(input_dim, input_dim)\n",
        "        self.linear4 = nn.Linear(input_dim, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.relu(self.linear3(x))\n",
        "        x = self.linear4(x)\n",
        "        return x        \n"
      ],
      "metadata": {
        "id": "iVNiG51OSwVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP + Encoder Architecture"
      ],
      "metadata": {
        "id": "wcivd0gkw8SN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MLPEncoder, self).__init__()\n",
        "        self.encoder = Encoder(lat_dim=20)\n",
        "        self.mlp = Mlp()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "xb1ucB0Kw_6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train MLP + Encoder"
      ],
      "metadata": {
        "id": "GN23vlTimxGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(model, train_loader, trainset, encoder, criterion, optimizer):\n",
        "    model.train()\n",
        "    loss_counter = 0.0\n",
        "    acc_counter = 0.0\n",
        "    counter = 0\n",
        "    for i, data in tqdm(enumerate(train_loader), total=int(len(trainset)/train_loader.batch_size)):\n",
        "        img, labels = data\n",
        "        if encoder:\n",
        "            img = encoder(img)\n",
        "        predict = model(img)\n",
        "        loss = criterion(predict, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        loss_counter += loss.item()\n",
        "        acc_counter += calculate_accuracy(labels, predict).item()\n",
        "        counter += 1\n",
        "    return loss_counter, acc_counter, counter\n",
        "\n",
        "\n",
        "def eval_classifier(model, test_loader, testset, encoder, criterion):\n",
        "    model.eval()\n",
        "    loss_counter = 0.0\n",
        "    acc_counter = 0.0\n",
        "    counter = 0\n",
        "    for i, data in tqdm(enumerate(test_loader), total=int(len(testset) / test_loader.batch_size)):\n",
        "        img, labels = data\n",
        "        if encoder:\n",
        "            img = encoder(img)\n",
        "        predict = model(img)\n",
        "        loss = criterion(predict, labels)\n",
        "        loss_counter += loss.item()\n",
        "        acc_counter += calculate_accuracy(labels, predict).item()\n",
        "        counter += 1\n",
        "    return loss_counter, acc_counter, counter\n",
        "\n",
        "\n",
        "def classifier(model, num_epochs=10, learning_rate=1e-3, trainset=trainset_mlp, train_loader=train_loader_mlp, model_name='Mlp'):\n",
        "    loss_list = []\n",
        "    acc_list = []\n",
        "    loss_list_test = []\n",
        "    acc_list_test = []\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.CrossEntropyLoss()  # cross entropy loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    encoder = None\n",
        "    if model_name == 'Mlp':\n",
        "        encoder = Encoder(lat_dim=20)\n",
        "        encoder.load_state_dict(copy.deepcopy(torch.load(\"encoder_model_20.pth\", device)))\n",
        "    for epoch in range(num_epochs):\n",
        "        loss_counter, acc_counter, counter = train_classifier(model, train_loader, trainset, encoder, criterion, optimizer)\n",
        "        loss_counter_test, acc_counter_test, counter_test = eval_classifier(model, testloader, testset, encoder, criterion)\n",
        "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, loss_counter / counter))\n",
        "        print('Epoch:{}, acc:{:.4f}'.format(epoch+1, acc_counter / counter))\n",
        "        print('Epoch:{}, Loss test:{:.4f}'.format(epoch+1, loss_counter_test / counter_test))\n",
        "        print('Epoch:{}, acc test:{:.4f}'.format(epoch+1, acc_counter_test / counter_test))\n",
        "        loss_list.append(loss_counter / counter)\n",
        "        acc_list.append(100 * acc_counter / counter)\n",
        "        loss_list_test.append(loss_counter_test / counter_test)\n",
        "        acc_list_test.append(100 * acc_counter_test / counter_test)\n",
        "    plt.plot(loss_list)\n",
        "    plt.plot(loss_list_test)\n",
        "    plt.title('loss')\n",
        "    plt.legend(['train', 'test'])\n",
        "    plt.show()\n",
        "    plt.plot(acc_list)\n",
        "    plt.plot(acc_list_test)\n",
        "    plt.title('accuracy')\n",
        "    plt.legend(['train', 'test'])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6yyjNkQSm1om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_classifiers():\n",
        "    model = Mlp()\n",
        "    max_epochs = 20\n",
        "    outputs = classifier(model, num_epochs=max_epochs, learning_rate=0.02, model_name='Mlp')\n",
        "\n",
        "    model = MLPEncoder()\n",
        "    max_epochs = 30\n",
        "    outputs = classifier(model, num_epochs=max_epochs, learning_rate=0.01, model_name='MLPEncoder')"
      ],
      "metadata": {
        "id": "0FJWMBP_B7OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_classifiers()"
      ],
      "metadata": {
        "id": "yOMcGpytF_ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Encoder and Decoder models for the next tasks"
      ],
      "metadata": {
        "id": "pn9rTo7PPgSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(lat_dim=20)\n",
        "decoder.load_state_dict(copy.deepcopy(torch.load(\"decoder_model_20.pth\", device)))\n",
        "decoder.eval()\n",
        "\n",
        "encoder = Encoder(lat_dim=20)\n",
        "encoder.load_state_dict(copy.deepcopy(torch.load(\"encoder_model_20.pth\", device)))\n",
        "encoder.eval()"
      ],
      "metadata": {
        "id": "3SmW2QFPPtPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN Architecture"
      ],
      "metadata": {
        "id": "aCNhPGtAyv5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1r87ZtznPsZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim=20):\n",
        "        super(Generator, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, 1024)\n",
        "        self.linear2 = nn.Linear(1024, 512)\n",
        "        self.linear3 = nn.Linear(512, 256)\n",
        "        self.linear4 = nn.Linear(256, 20)\n",
        "        self.batch_norm_1 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm_2 = nn.BatchNorm1d(256)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.batch_norm_1(self.linear2(x)))\n",
        "        x = self.relu(self.batch_norm_2(self.linear3(x)))\n",
        "        x = self.linear4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.linear1 = nn.Linear(20, 1024)\n",
        "        self.linear2 = nn.Linear(1024, 512)\n",
        "        self.linear3 = nn.Linear(512, 256)\n",
        "        self.linear4 = nn.Linear(256, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.relu(self.linear3(x))\n",
        "        x = self.sigmoid(self.linear4(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "5FvZIMqoyy-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train GAN"
      ],
      "metadata": {
        "id": "ORisDuMu975K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_latent(number_of_images=batch_size, latent_dimension=latent_dim):\n",
        "    \"\"\" Generate noise for number_of_images images, with a specific noise_dimension \"\"\"\n",
        "    return torch.randn(number_of_images, latent_dimension)\n",
        "\n",
        "\n",
        "def generate_specific_digit(number_of_images=batch_size, digit_to_generate=0):\n",
        "    zeros = torch.zeros(number_of_images)\n",
        "    zeros[:] = digit_to_generate\n",
        "    return zeros.to(torch.int64)\n",
        "\n",
        "\n",
        "def initialize_optimizers(generator, discriminator, lr_genrator, lr_discriminator):\n",
        "    \"\"\" Initialize optimizers for Generator and Discriminator. \"\"\"\n",
        "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=lr_genrator, betas=(0.5, 0.999))\n",
        "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr_discriminator, betas=(0.5, 0.999))\n",
        "    return generator_optimizer, discriminator_optimizer\n",
        "\n",
        "\n",
        "def show_generated_images(generator):\n",
        "    decoder = Decoder(lat_dim=20)\n",
        "    decoder.load_state_dict(copy.deepcopy(torch.load(\"decoder_model_20.pth\", device)))\n",
        "\n",
        "    latents = generate_latent()\n",
        "    generate_images = generator(latents)\n",
        "    outputs = decoder(generate_images[:16])\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    fig = plt.gcf()\n",
        "    fig.set_size_inches(6, 6)\n",
        "    recon = outputs.detach().numpy()\n",
        "    for i, item in enumerate(recon):\n",
        "        if i >= 16: break\n",
        "        plt.subplot(4, 4, 0+i+1)\n",
        "        plt.imshow(item[0], cmap='gray')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WyNYkyUY6j2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_gan(generator, discriminator, encoder, loss_function, generator_optimizer, discriminator_optimizer):\n",
        "    real_label, fake_label = 1.0, 0.0\n",
        "    loss_dis = 0.0\n",
        "    loss_gen = 0.0\n",
        "    counter = 0\n",
        "    for i, data in tqdm(enumerate(trainloader), total=int(len(trainset)/trainloader.batch_size)):\n",
        "        # train discriminator\n",
        "        discriminator.zero_grad()\n",
        "        img, _ = data\n",
        "        real_images = encoder(img)\n",
        "        label = torch.full((img.shape[0],1), real_label)\n",
        "        output = discriminator(real_images)\n",
        "        error_real_images = loss_function(output, label)\n",
        "        error_real_images.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        latent = generate_latent(img.shape[0])\n",
        "        fake = generator(latent)\n",
        "        label.fill_(fake_label)\n",
        "        output = discriminator(fake.detach())\n",
        "        error_fake_images = loss_function(output, label)\n",
        "        error_fake_images.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        errD = error_real_images + error_fake_images\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # train generator\n",
        "        generator.zero_grad()\n",
        "        label.fill_(real_label)\n",
        "        output = discriminator(fake)\n",
        "        errG = loss_function(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        loss_dis += errD\n",
        "        loss_gen += errG\n",
        "        counter += 1\n",
        "    return loss_dis / counter, loss_gen / counter\n",
        "\n",
        "\n",
        "def train_gan(generator, discriminator, num_epochs=30, lr_discriminator=1e-3, lr_generator=1e-3, trainset=trainset, train_loader=trainloader):\n",
        "    loss_discriminator = []\n",
        "    loss_generator = []\n",
        "    torch.manual_seed(42)\n",
        "    encoder = Encoder(lat_dim=20)\n",
        "    encoder.load_state_dict(copy.deepcopy(torch.load(\"encoder_model_20.pth\", device)))\n",
        "    criterion = nn.BCELoss()\n",
        "    generator_optimizer, discriminator_optimizer = initialize_optimizers(generator, discriminator, lr_generator, lr_discriminator)\n",
        "    for epoch in range(num_epochs):\n",
        "        loss_dis, loss_gen = train_epoch_gan(generator, discriminator, encoder, criterion, generator_optimizer, discriminator_optimizer)\n",
        "        print('Epoch:{}, Loss discriminator:{:.4f}'.format(epoch+1, loss_dis))\n",
        "        print('Epoch:{}, Loss generator:{:.4f}'.format(epoch+1, loss_gen))\n",
        "        loss_discriminator.append(loss_dis)\n",
        "        loss_generator.append(loss_gen)\n",
        "        show_generated_images(generator)\n",
        "        plt.plot(loss_discriminator)\n",
        "        plt.plot(loss_generator)\n",
        "        plt.title('GAN loss')\n",
        "        plt.legend(['discriminator', 'generator'])\n",
        "        plt.show()\n",
        "    torch.save(generator.state_dict(), 'generator.pth')"
      ],
      "metadata": {
        "id": "6kOIGxqo9-BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gan():\n",
        "    batch_size=128\n",
        "    generator = Generator()\n",
        "    discriminator = Discriminator()\n",
        "    num_epochs = 100\n",
        "    train_gan(generator, discriminator, num_epochs, lr_discriminator=0.0002, lr_generator=0.0002)\n",
        "\n",
        "    # interpolation experiment - GAN\n",
        "    print('interpolation experiment - GAN')\n",
        "    latents = generate_latent()\n",
        "    for a in range(11):\n",
        "        a = a / 10\n",
        "        generator.eval()\n",
        "        interpolation = a*latents[:1] + (1-a)*latents[1:2]\n",
        "        genetared = generator(interpolation)\n",
        "        outputs = decoder(genetared.reshape((1, 20)))\n",
        "        plt.figure(figsize=(4, 4))\n",
        "        fig = plt.gcf()\n",
        "        fig.set_size_inches(6, 6)\n",
        "        recon = outputs.detach().numpy()\n",
        "        for i, item in enumerate(recon):\n",
        "            if i >= 16: break\n",
        "            plt.subplot(4, 4, 0+i+1)\n",
        "            plt.imshow(item[0], cmap='gray')\n",
        "        plt.show()\n",
        "\n",
        "    # interpolation experiment - AE\n",
        "    print('interpolation experiment - AE')\n",
        "    for i, data in enumerate(train_loader_two_images):\n",
        "        img, labels = data\n",
        "        l1 = img[0]\n",
        "        l2 = img[1]\n",
        "        for a in range(11):\n",
        "            a = a / 10\n",
        "            interpolation = a*encoder(l1.reshape(1, 1, 32, 32)) + (1-a)*encoder(l2.reshape(1, 1, 32, 32))\n",
        "            interpolation = decoder(interpolation)\n",
        "            plt.imshow(interpolation.detach().numpy().reshape((32, 32)), cmap='gray')\n",
        "            plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "pEmULqIgHsP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_gan()"
      ],
      "metadata": {
        "id": "9UvGvsssIFBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional GAN"
      ],
      "metadata": {
        "id": "ZfkDfBTeWklh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalGenerator(nn.Module):\n",
        "    def __init__(self, input_dim=20):\n",
        "        super(ConditionalGenerator, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim+10, 1024)\n",
        "        self.linear2 = nn.Linear(1024, 512)\n",
        "        self.linear3 = nn.Linear(512, 256)\n",
        "        self.linear4 = nn.Linear(256, 20)\n",
        "        self.embedding = nn.Embedding(10, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        c = self.embedding(labels)\n",
        "        x = torch.cat([x, c], 1)\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.relu(self.linear3(x))\n",
        "        x = self.linear4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConditionalDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConditionalDiscriminator, self).__init__()\n",
        "        self.linear1 = nn.Linear(20+10, 1024)\n",
        "        self.linear2 = nn.Linear(1024, 512)\n",
        "        self.linear3 = nn.Linear(512, 256)\n",
        "        self.linear4 = nn.Linear(256, 1)\n",
        "        self.embedding = nn.Embedding(10, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        c = self.embedding(labels)\n",
        "        x = torch.cat([x, c], 1)\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.relu(self.linear2(x))\n",
        "        x = self.relu(self.linear3(x))\n",
        "        x = self.sigmoid(self.linear4(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "tgZJ8zK3QZ8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Conditional GAN"
      ],
      "metadata": {
        "id": "xq-iejROyO9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fake_labels(number_of_images=batch_size):\n",
        "    return torch.randint(0, 10, (number_of_images,))\n",
        "\n",
        "\n",
        "def show_cgan_images(generator, digits=[2]):\n",
        "     for digit in digits:\n",
        "        print('digit: ', digit)\n",
        "        labels = generate_specific_digit(digit_to_generate=digit)\n",
        "        latents = generate_latent()\n",
        "        generate_images = generator(latents, labels)\n",
        "        # print(\"Generate images: \", generate_images)\n",
        "        outputs = decoder(generate_images[:16])\n",
        "\n",
        "        plt.figure(figsize=(4, 4))\n",
        "        fig = plt.gcf()\n",
        "        fig.set_size_inches(6, 6)\n",
        "        recon = outputs.detach().numpy()\n",
        "        for i, item in enumerate(recon):\n",
        "            if i >= 16: break\n",
        "            plt.subplot(4, 4, 0+i+1)\n",
        "            plt.imshow(item[0], cmap='gray')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def train_epoch_cgan(generator, discriminator, encoder, loss_function, generator_optimizer, discriminator_optimizer):\n",
        "    real_label, fake_label = 1.0, 0.0\n",
        "    loss_dis = 0.0\n",
        "    loss_gen = 0.0\n",
        "    counter = 0\n",
        "    for i, data in tqdm(enumerate(trainloader), total=int(len(trainset)/trainloader.batch_size)):\n",
        "        # train discriminator\n",
        "        discriminator.zero_grad()\n",
        "        img, digit_labels = data\n",
        "        real_images = encoder(img)\n",
        "        label = torch.full((img.shape[0],1), real_label)\n",
        "        output = discriminator(real_images, digit_labels)\n",
        "        error_real_images = loss_function(output, label)\n",
        "        error_real_images.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        latent = generate_latent(img.shape[0])\n",
        "        fake_digit_label = generate_fake_labels(img.shape[0])\n",
        "        fake = generator(latent, fake_digit_label)\n",
        "        output = discriminator(fake.detach(), fake_digit_label)\n",
        "        label.fill_(fake_label)\n",
        "        error_fake_images = loss_function(output, label)\n",
        "        error_fake_images.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        errD = error_real_images + error_fake_images\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # train generator\n",
        "        generator.zero_grad()\n",
        "        generated_data = generator(latent, fake_digit_label)\n",
        "        output = discriminator(generated_data, fake_digit_label)\n",
        "        label.fill_(real_label)\n",
        "        errG = loss_function(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        loss_dis += errD\n",
        "        loss_gen += errG\n",
        "        counter += 1\n",
        "    return loss_dis / counter, loss_gen / counter\n",
        "\n",
        "\n",
        "def train_cgan(generator, discriminator, num_epochs=30, lr_discriminator=1e-3, lr_generator=1e-3, trainset=trainset, train_loader=trainloader):\n",
        "    loss_discriminator = []\n",
        "    loss_generator = []\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.BCELoss()\n",
        "    generator_optimizer, discriminator_optimizer = initialize_optimizers(generator, discriminator, lr_generator, lr_discriminator)\n",
        "    for epoch in range(num_epochs):\n",
        "        loss_dis, loss_gen = train_epoch_cgan(generator, discriminator, encoder, criterion, generator_optimizer, discriminator_optimizer)\n",
        "        print('Epoch:{}, Loss discriminator:{:.4f}'.format(epoch+1, loss_dis))\n",
        "        print('Epoch:{}, Loss generator:{:.4f}'.format(epoch+1, loss_gen))\n",
        "        loss_discriminator.append(loss_dis)\n",
        "        loss_generator.append(loss_gen)\n",
        "        show_cgan_images(generator)\n",
        "        plt.plot(loss_discriminator)\n",
        "        plt.plot(loss_generator)\n",
        "        plt.title('Conditional GAN loss')\n",
        "        plt.legend(['discriminator', 'generator'])\n",
        "        plt.show()\n",
        "        torch.save(generator.state_dict(), 'cond_generator.pth')\n"
      ],
      "metadata": {
        "id": "Va-nZ9n7yKQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cgan():\n",
        "    batch_size = 128\n",
        "    generator = ConditionalGenerator()\n",
        "    discriminator = ConditionalDiscriminator()\n",
        "    num_epochs = 1\n",
        "    train_cgan(generator, discriminator, num_epochs, lr_discriminator=0.0002, lr_generator=0.0002)\n",
        "    show_cgan_images(generator, list(range(10)))"
      ],
      "metadata": {
        "id": "dIxLSFMZQjwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_cgan()"
      ],
      "metadata": {
        "id": "3X2320JJQoms"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}